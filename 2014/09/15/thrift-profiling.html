<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>A python/thrift profiling story</title>
        <meta name="viewport" content="width=device-width">

        <!-- syntax highlighting CSS -->
        <link rel="stylesheet" href="/css/syntax.css">

        <!-- Custom CSS -->
        <link rel="stylesheet" href="/css/main.css">

        <!-- zoomingbox -->
        <script src="/assets/js/jquery-1.11.0.min.js"></script>
        <script src="/assets/js/jquery.zoomingbox.min.js"></script>
        <link href="/assets/css/zoomingbox.min.css" rel="stylesheet" />
    </head>
    <body>

        <div class="site">
          <div class="header">
            <h1 class="title"><a href="/">vigoo's software development blog</a></h1>
            <a class="extra" href="/">home</a>
          </div>

          <h2>A python/thrift profiling story</h2>
<p class="meta">15 Sep 2014</p>

<div class="post">
<p>A few weeks ago I met a problem where a script, running once every night sending out some emails did not run correctly because a remote thrift call timed out in it. As I started investigating it, turned out that it&#39;s a <em>search</em> call:</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">staff_users</span> <span class="o">=</span> <span class="n">RemoteUserFactory</span><span class="p">()</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">is_staff</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></code></pre></div>

<p>The details here are not really important, what this call does is that it asks a service to return a <em>set of users</em>, and the communication is going on <a href="https://thrift.apache.org/">thrift</a>.</p>

<p>Executing it manually on the server revealed that it should return <em>5649</em> users. Checking out the logs I could see that the call took extremely long time, between 8 to 12 seconds. Even when the cron job was moved from 3:00 AM to a less busy time (several other jobs were executing at the same time), it took more than 6 seconds!</p>

<p>This was suspicious so I also checked the log of a <em>proxy</em> which runs on the same host as the script itself and provides client side load balancing, circuit breaking, retry logic etc. for thrift connections. This log showed that the service replied in <em>2.5 seconds</em>, but it took almost 4 seconds to get this response from the proxy to the client on localhost! This seemed to be completely unacceptable, and also the 2.5 second response time from the service seemed to be too big (I ran the query on one of the server nodes and it returned the users from the database almost instantly). I also had similar experience (but without measurements) before.</p>

<p>So I decided to find out what&#39;s going on. And I found the process interesting enough to write this post about it :)</p>

<h2>Test environment</h2>

<p>I started by adding a test method to the service&#39;s thrift API called <code>test_get_users(count, sleep)</code> which returns <code>count</code> fake users after waiting <code>sleep</code> seconds. Then in the following experiments I called it with <code>(5499, 1)</code>. The 1 second sleep was intended to simulate the network latency and database query; there was no advantage from having it at the end, but as it is visible everywhere in the results, I had to mention.</p>

<p>For finding out what&#39;s going on I used <a href="https://docs.python.org/2/library/profile.html">cProfile</a> with <a href="https://code.google.com/p/jrfonseca/">gprof2dot</a>, calling the remote test method from a django shell, while everything is running on localhost.</p>

<h3>First measurement</h3>

<p>Without touching anything, returning 5499 dummy users on localhost took <strong>5.272 seconds</strong>!</p>

<p>The client side of the call looked like this:</p>

<p><a href="/assets/images/profile1.png" class="zimg"><img width="600" src="/assets/images/profile1.png" alt=""></a></p>

<p>Here we can see that the call has two major phases:</p>

<ul>
<li>The thrift call itself (65%)</li>
<li>Converting the raw results to model objects with <code>_row_to_model</code> (35%)</li>
</ul>

<p>Let&#39;s see first the thrift call (the green branch on the picture). Once again it has two, nearly equivalent branches:</p>

<ul>
<li><code>send_test_get_users</code> which sends the request and waits for the response. This includes the 1 second sleep as well.</li>
<li><code>recv_test_get_users</code> processes the response</li>
</ul>

<p>What&#39;s interesting here is that <code>recv_test_get_users</code> took ~32% of the overall time which is around ~1.6 seconds for simple data deserialization.</p>

<h3>Optimizing thrift deserialization</h3>

<p>I did not want to believe that the python thrift deserialization is that slow, so I did a search and found that the <code>TBinaryProtocol</code> which we are using is really that slow.</p>

<p>But the thrift library contains a class called <code>TBinaryProtocolAccelerated</code> which is about 10x faster (according to a stackoverflow post).</p>

<p>First I simply changed the used protocol to this, but nothing happened. Digging deeper I found that this is not a real protocol implementation, but a lower level hack.</p>

<p>The documentation of the protocol class says:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">  C-Accelerated version of TBinaryProtocol.

  This class does not override any of TBinaryProtocol&#39;s methods,
  but the generated code recognizes it directly and will call into
  our C module to do the encoding, bypassing this object entirely.
  We inherit from TBinaryProtocol so that the normal TBinaryProtocol
  encoding can happen if the fastbinary module doesn&#39;t work for some
  reason.  (TODO(dreiss): Make this happen sanely in more cases.)

  In order to take advantage of the C module, just use
  TBinaryProtocolAccelerated instead of TBinaryProtocol.
</code></pre></div>
<p>So why didn&#39;t it work? The answer is in <a href="https://github.com/apache/thrift/blob/master/lib/py/src/protocol/TBase.py#L52-L58">TBase.py</a>.</p>

<p>The following conditions have to met in order to use the fast deserializer:</p>

<ul>
<li>Protocol must be <code>TBinaryProtocolAccelerated</code> (I changed that)</li>
<li>Protocol&#39;s transport implementation must implement the <code>TTransport.CReadableTransport</code> interface</li>
<li><code>thrift_spec</code> must be available (this was true in this case)</li>
<li><code>fastbinary</code> must be available (also true)</li>
</ul>

<p>The problem was that we were replacing the <code>TTransport</code> implementation with a custom class called <code>ThriftifyTransport</code> in order to do thrift logging, HMAC authentication, etc.</p>

<p>Fortunately all the default transport implementations implement the <code>CReadableTransport</code> interface, and one of them, <code>TBufferedTransport</code> can be used to wrap another transport to add buffering around it. That&#39;s what I did, and it immediately started using the fast deserialization code.</p>

<p>The test call now ran in <strong>3.624 seconds</strong>.</p>

<p>And the new profiling results with this change:</p>

<p><a href="/assets/images/profile2.png" class="zimg"><img width="600" src="/assets/images/profile2.png" alt=""></a></p>

<p>The left-hand side of the call graph remained the same, but <code>recv_test_get_users</code> is now only 2.35% of the overall time which is ~0.08 seconds (to be compared with the 1.6 seconds with the original deserializer!)</p>

<h3>Optimizing thrift serialization</h3>

<p>The obvious next step was to apply this change on the server side as well, so our service can use the fast binary protocol for serialization too. For this I simply copied the change and remeasured everything.</p>

<p>The test call now ran in <strong>3.328 seconds</strong>!</p>

<p>Let&#39;s see the call graph of this stage:</p>

<p><a href="/assets/images/profile3.png" class="zimg"><img width="600" src="/assets/images/profile3.png" alt=""></a></p>

<h3>Optimizing result processing</h3>

<p>The client side of the test method was written similar to how the original API method is written:</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">test_get_users_thrift</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">sleep</span><span class="p">):</span>
    <span class="n">rpc</span> <span class="o">=</span> <span class="n">ThriftRPC</span><span class="p">(</span><span class="n">UserDataService</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">service_name</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">service_name</span><span class="p">,</span> <span class="n">client_config</span><span class="o">=</span><span class="n">client_config</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rpc</span><span class="o">.</span><span class="n">test_get_users</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">sleep</span><span class="p">)</span><span class="o">.</span><span class="n">iteritems</span><span class="p">():</span>
        <span class="n">user</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_row_to_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">user_factory</span><span class="p">,</span> <span class="n">row</span><span class="p">)</span>
        <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">user</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">result</span></code></pre></div>

<p>It is clearly visible on the call graph that the 5499 call to <code>_row_to_model</code> takes 53% of the total time, which is ~1.7 seconds. There are two main branches of this call. The left hand side (<code>row_to_model</code>) seemed to be simple data conversion, and its slowest part is date-time deserialization.</p>

<p>The other branch however looked like a real problem; why should we resolve HMAC host, or parse configuration for each row?</p>

<p>It turned out to be a bug, <code>_row_to_model</code> created a new <em>model factory</em> in each call, which involves a lot of initialization, config parsing, and similar things.</p>

<p>So the simple fix was to create a <code>_rows_to_model</code> helper function which does the same for multiple rows with a single factory.</p>

<p>Running my test code once again showed that the optimization makes sense. Now it ran in <strong>2.448 seconds</strong>, with the following call graph:</p>

<p><a href="/assets/images/profile4.png" class="zimg"><img width="600" src="/assets/images/profile4.png" alt=""></a></p>

<h3>Further optimizations</h3>

<p>I saw two possible ways to further optimize this case:</p>

<ol>
<li><p>Lazy conversion of raw thrift data to model data (per field). This would make sense because many times only a few fields (the id for example) are used, but it seemed to be a too complex change</p></li>
<li><p>Checking the server side as well</p></li>
</ol>

<p>To profile the server side and only measure the thrift request processing I had to add profiling code to the django view class in the following way:</p>

<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">cProfile</span>

<span class="n">cProfile</span><span class="o">.</span><span class="n">runctx</span><span class="p">(</span><span class="s">&#39;self._call_processor(op_data)&#39;</span><span class="p">,</span> <span class="nb">globals</span><span class="p">(),</span> <span class="nb">locals</span><span class="p">(),</span> <span class="s">&#39;callstats&#39;</span><span class="p">)</span>
<span class="c"># self._call_processor(op_data)</span></code></pre></div>

<p>The server-side call took <strong>1.691 seconds</strong> and looked like this:</p>

<p><a href="/assets/images/profile5.png" class="zimg"><img width="600" src="/assets/images/profile5.png" alt=""></a></p>

<p>As expected, 60% of this was the 1 second sleep. The rest of the calls are data conversion with no obvious point to improve.</p>

<h2>Summary</h2>

<p>These optimizations are decreasing the response time significantly, especially for calls returning multiple rows.</p>

<p>The interesting was that the extremely slow performance was caused by both the slow perfomance of the python thrift serializer and a bug in our code.</p>

</div>

<div id="disqus_thread"></div>
<script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'vigoo'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    


          <div class="footer">
            <div class="contact">
              <p>
                Daniel Vigovszky<br />
              </p>
            </div>
            <div class="contact">
              <p>
                <a href="https://github.com/vigoo">github.com/vigoo</a><br />
              </p>
            </div>
          </div>
        </div>

    </body>
    <script type="text/javascript">
      $('.zimg').zoomingBox();
    </script>
</html>
